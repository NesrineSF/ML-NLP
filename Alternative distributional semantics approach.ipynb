{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define credentials inside your code\n",
    "import os\n",
    "os.environ[\"EAI_USERNAME\"] = 'YOUR_USER'\n",
    "os.environ[\"EAI_PASSWORD\"] = 'YOUR_PASSWORD'\n",
    "\n",
    "#import the client section of the library\n",
    "from expertai.nlapi.cloud.client import ExpertAiClient\n",
    "client = ExpertAiClient()\n",
    "\n",
    "###################\n",
    "##Text Subdivison##\n",
    "###################\n",
    "\n",
    "\n",
    "#The text to process, the language of the text and the parameters of the API\n",
    "text = \"Sophia is a social humanoid robot developed by Hong Kong-based company Hanson Robotics. Sophia was activated on February 14, 2016.\" \n",
    "language= 'en'\n",
    "\n",
    "output = client.specific_resource_analysis(\n",
    "    body={\"document\": {\"text\": text}}, \n",
    "    params={'language': language, 'resource': 'disambiguation'\n",
    "})\n",
    "\n",
    "#________Text subdivison into paragraphs________##\n",
    "\n",
    "#we use the element output.paragraph to subdivise the text into paragraph\n",
    "#start and end are the positions of the paragraph in the analyzed text\n",
    "for paragraph in output.paragraphs:\n",
    "    print (f'Paragraphs:{text[paragraph.start:paragraph.end]:{20}}')\n",
    "    \n",
    "    \n",
    "#________Text subdivison into sentences________##\n",
    "\n",
    "#we use the element output.sentences to subdivise the text into sentences\n",
    "#start and end are the positions of the sentence in the analyzed text\n",
    "for sentence in output.sentences:\n",
    "    print (f'Sentences:{text[sentence.start:sentence.end]:{20}}')\n",
    "\n",
    "\n",
    "#________Text subdivison into phrases________##\n",
    "\n",
    "#we use the element output.phrases to subdivise the text into phrases\n",
    "#start and end are the positions of the phrase in the analyzed text\n",
    "for phrase in output.phrases:\n",
    "    print (f'Phrases:{text[phrase.start:phrase.end]:{20}}')\n",
    "    \n",
    "    \n",
    "    \n",
    "################\n",
    "##Tokenization##\n",
    "################\n",
    "\n",
    "#split() function to tokenize the sentence\n",
    "\n",
    "text = \"CNBC has commented on the robot's  lifelike skin and her ability to emulate more than 60 facial expressions.\"\n",
    "\n",
    "tokens = text.split()\n",
    "print('These are the tokens of the sentence', tokens)\n",
    "\n",
    "\n",
    "\n",
    "#________Text subdivison into tokens with NL API________##\n",
    "\n",
    "\n",
    "text = \"CNBC has commented on the robot's  lifelike skin and her ability to emulate more than 60 facial expressions.\" \n",
    "language= 'en'\n",
    "\n",
    "output = client.specific_resource_analysis(\n",
    "    body={\"document\": {\"text\": text}}, \n",
    "    params={'language': language, 'resource': 'disambiguation'\n",
    "})\n",
    "\n",
    "#to print to tokens within the sentence\n",
    "print (f'{\"TOKEN\":{20}} ')\n",
    "print (f'{\"----\":{20}}') \n",
    "\n",
    "#we use the element output.tokens to subdivise the text into tokens\n",
    "#start and end are the positions of the token in the analyzed text\n",
    "for token in output.tokens:\n",
    "    print (f'{text[token.start:token.end]:{20}}')\n",
    "\n",
    "    \n",
    "#________Tokenization into atom level________##\n",
    "for token in output.tokens:\n",
    "    print (f'{text[token.start:token.end]:{20}}')\n",
    "    #we iterate on the tokens array to subdivise the compound words into the atom level\n",
    "    for atom in token.atoms:\n",
    "        print (f'\\t atom:{text[atom.start:atom.end]:{20}}')\n",
    "\n",
    "\n",
    "###############\n",
    "##PoSÂ Tagging##\n",
    "###############\n",
    "\n",
    "\n",
    "#Declare the variables related to each sentence\n",
    "object_noun = \"The object of this exercise is to raise money for the charity.\"\n",
    "object_verb = \"A lot of people will object to the book.\"\n",
    "\n",
    "#_______________Request of the API for both sentences_________#\n",
    "\n",
    "#first sentence\n",
    "object_noun = \"The object of this exercise is to raise money for the charity.\"\n",
    "#second sentence\n",
    "object_verb = \"A lot of people will object to the book.\"\n",
    "language = 'en'\n",
    "#Request to the API for the first sentence\n",
    "output = client.specific_resource_analysis(\n",
    "    body={\"document\": {\"text\": object_noun}}, \n",
    "    params={'language': language, 'resource': 'disambiguation'\n",
    "})\n",
    "\n",
    "#Request to the API for the second sentence\n",
    "output_2 = client.specific_resource_analysis(\n",
    "    body={\"document\": {\"text\": object_verb}}, \n",
    "    params={'language': language, 'resource': 'disambiguation'\n",
    "})\n",
    "\n",
    "#__________iteration over the tokens of each sentence to assign a POS to each token____________#\n",
    "\n",
    "#POS of the first sentence\n",
    "print (f'\\t \\033[1mOutput of the first sentence : \\033[0m \\n') \n",
    "#to print TOKEN and POS in bold at the beginning of the output array:\n",
    "print (f'\\033[1m{\"TOKEN\":{20}} {\"POS\":{6}}\\033[0m')\n",
    "\n",
    "#to iterate over the tokens and assign a POS to each token for the first sentence\n",
    "for token in output.tokens:\n",
    "    print (f'{object_noun[token.start:token.end]:{20}} {token.pos:{6}}')\n",
    "\n",
    "    \n",
    "    \n",
    "#POS of the second sentence\n",
    "print (f'\\t \\033[1mOutput of the second sentence : \\033[0m \\n') \n",
    "#to print TOKEN and POS in bold at the beginning of the output array:\n",
    "print (f'\\033[1m{\"TOKEN\":{20}} {\"POS\":{6}}\\033[0m')\n",
    "\n",
    "#to iterate over the tokens and assign a POS to each token for the second sentence\n",
    "for token in output_2.tokens:\n",
    "    print (f'{object_verb[token.start:token.end]:{20}} {token.pos:{6}}')\n",
    "    \n",
    "\n",
    "#__________concept_ID for each word \"object\" within the knowledge graph_______#\n",
    "\n",
    "    #______When NOUN________#\n",
    "    \n",
    "#Concept_ID for object_noun\n",
    "print (f'\\t \\033[1mConcept_ID for object when NOUN \\033[0m \\n') \n",
    "\n",
    "#to print TOKEN, POS  and ID (for the concept) in bold at the beginning of the output array:\n",
    "print (f'\\033[1m{\"TOKEN\":{20}} {\"POS\":{15}} {\"ID\":{6}}\\033[0m')\n",
    "\n",
    "#Syncon stands for \"Concept\" that we refer to with an ID\n",
    "for token in output.tokens:\n",
    "    print (f'{object_noun[token.start:token.end]:{20}} {token.pos:{15}} {token.syncon:{6}} ')\n",
    "\n",
    "    #______When VERB________#\n",
    "    \n",
    "#Concept_ID for object_verb\n",
    "print (f'\\t \\033[1mConcept_ID for object when VERB \\033[0m \\n') \n",
    "\n",
    "#to print TOKEN, POS  and ID (for the concept) in bold at the beginning of the output array:\n",
    "print (f'\\033[1m{\"TOKEN\":{20}} {\"POS\":{15}} {\"ID\":{6}}\\033[0m')\n",
    "\n",
    "#Syncon stands for \"Concept\" that we refer to with an ID\n",
    "for token in output_2.tokens:\n",
    "    print (f'{object_verb[token.start:token.end]:{20}} {token.pos:{15}} {token.syncon:{6}} ')\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "###############\n",
    "#Lemmatization#\n",
    "###############\n",
    "\n",
    "#_______same word belonging to different lemma_____#\n",
    "\n",
    "living_from_live = \"She's living her best life\"\n",
    "living_from_living = \"What do you do for a living?\"\n",
    "\n",
    "language = 'en'\n",
    "\n",
    "#Request to the API_First sentence\n",
    "output = client.specific_resource_analysis(\n",
    "    body={\"document\": {\"text\": living_from_live }}, \n",
    "    params={'language': language, 'resource': 'disambiguation'\n",
    "})\n",
    "\n",
    "#Request to the API_Second sentence\n",
    "output_2 = client.specific_resource_analysis(\n",
    "    body={\"document\": {\"text\": living_from_living }}, \n",
    "    params={'language': language, 'resource': 'disambiguation'\n",
    "})\n",
    "\n",
    "#to print TOKEN, POS  and ID (for the concept) in bold at the beginning of the output array:\n",
    "print (f'\\t \\033[1mOuput of the first sentence : \\033[0m \\n') \n",
    "print (f'\\033[1m{\"TOKEN\":{20}} {\"LEMMA\":{15}} {\"POS\":{6}} \\033[0m')\n",
    "\n",
    "#Syncon stands for \"Concept\" that we refer to with an ID\n",
    "for token in output.tokens:\n",
    "    print (f'{living_from_live [token.start:token.end]:{20}} {token.lemma:{15}} {token.pos:{6}} ')\n",
    "    \n",
    "print (f'\\t \\033[1mOuput of the second sentence : \\033[0m \\n')   \n",
    "print (f'\\033[1m{\"TOKEN\":{20}} {\"LEMMA\":{15}} {\"POS\":{6}} \\033[0m')\n",
    "for token in output_2.tokens:\n",
    "    print (f'{living_from_living [token.start:token.end]:{20}} {token.lemma:{15}} {token.pos:{6}} ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
